{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook generates the tree classifier model used in the streamlit demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv data into pandas dataframe\n",
    "df = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    891 non-null    int64  \n",
      " 1   Sex       891 non-null    object \n",
      " 2   Age       714 non-null    float64\n",
      " 3   SibSp     891 non-null    int64  \n",
      " 4   Parch     891 non-null    int64  \n",
      " 5   Fare      891 non-null    float64\n",
      " 6   Embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 48.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data by separating X and y\n",
    "# dropping unimportant features, such as passenger id, name, ticket number and cabin number\n",
    "# note that interesting features might be engieered from the dropped features above\n",
    "\n",
    "# axis = 1 below means dropping by columns, 0 means by rows\n",
    "X = df.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "y = df['Survived']\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 7)\n",
      "(179, 7)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training set and a test set. \n",
    "# Any number for the random_state is fine, see 42: https://en.wikipedia.org/wiki/42_(number) \n",
    "# We choose to use 20% (test_size=0.2) of the data set as the test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train our decision tree classifier with the following features:\n",
    "# Numerical Features: ['Age', 'SibSp', 'Fare']\n",
    "# Categorical Features:['Sex', 'Embarked', 'Pclass'\n",
    "\n",
    "num_features = ['Age', 'SibSp', 'Fare']\n",
    "cat_features = ['Sex', 'Pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Create the preprocessing pipeline for numerical features\n",
    "# There are two steps in this pipeline\n",
    "# Pipeline(steps=[(name1, transform1), (name2, transform2), ...]) \n",
    "# NOTE the step names can be arbitrary\n",
    "\n",
    "# Step 1 is what we discussed before - filling the missing values if any using mean\n",
    "# Step 2 is feature scaling via standardization - making features look like normal-distributed \n",
    "# see sandardization: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "num_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('num_imputer', SimpleImputer()),  # we will tune differet strategies later\n",
    "        ('scaler', StandardScaler()),\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Create the preprocessing pipelines for the categorical features\n",
    "# There are two steps in this pipeline:\n",
    "# Step 1: filling the missing values if any using the most frequent value\n",
    "# Step 2: one hot encoding\n",
    "\n",
    "cat_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Assign features to the pipelines and Combine two pipelines to form the preprocessor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_pipeline', num_pipeline, num_features),\n",
    "        ('cat_pipeline', cat_pipeline, cat_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model to use, which is DecisionTreeClassifier\n",
    "# Make a full pipeline by combining preprocessor and the model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline_dt = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf_dt', DecisionTreeClassifier()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we show how to use GridSearch with K-fold cross validation (K=10) to fine tune the model\n",
    "# we use the accuracy as the scoring metric with training score return_train_score=True\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set up the values of hyperparameters you want to evaluate\n",
    "# here you must use the step names as the prefix followed by two under_scores to sepecify the parameter names and the \"full path\" of the steps\n",
    "\n",
    "# we are trying 2 different impputer strategies \n",
    "# 2x5 different decision tree models with different parameters\n",
    "# in total we are trying 2x2x5 = 20 different combinations\n",
    "\n",
    "param_grid_dt = [\n",
    "    {\n",
    "        'preprocessor__num_pipeline__num_imputer__strategy': ['mean', 'median'],\n",
    "        'clf_dt__criterion': ['gini', 'entropy'], \n",
    "        'clf_dt__max_depth': [3, 4, 5, 6, 7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# set up the grid search \n",
    "grid_search_dt = GridSearchCV(pipeline_dt, param_grid_dt, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('num_pipeline',\n",
       "                                                                         Pipeline(steps=[('num_imputer',\n",
       "                                                                                          SimpleImputer()),\n",
       "                                                                                         ('scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['Age',\n",
       "                                                                          'SibSp',\n",
       "                                                                          'Fare']),\n",
       "                                                                        ('cat_pipeline',\n",
       "                                                                         Pipeline(steps=[('cat_imputer',\n",
       "                                                                                          SimpleImputer(strategy='most_frequent')),\n",
       "                                                                                         ('onehot',\n",
       "                                                                                          OneHotEncoder())]),\n",
       "                                                                         ['Sex',\n",
       "                                                                          'Pclass'])])),\n",
       "                                       ('clf_dt', DecisionTreeClassifier())]),\n",
       "             param_grid=[{'clf_dt__criterion': ['gini', 'entropy'],\n",
       "                          'clf_dt__max_depth': [3, 4, 5, 6, 7],\n",
       "                          'preprocessor__num_pipeline__num_imputer__strategy': ['mean',\n",
       "                                                                                'median']}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using the full pipeline\n",
    "grid_search_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf_dt__criterion': 'entropy',\n",
       " 'clf_dt__max_depth': 3,\n",
       " 'preprocessor__num_pipeline__num_imputer__strategy': 'mean'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the best performing parameter combination\n",
    "grid_search_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8258411580594679"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best decistion tree model test score\n",
    "grid_search_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = grid_search_dt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "# final test on the testing set\n",
    "# To predict on new data: simply calling the predict method \n",
    "# the full pipeline steps will be applied to the testing set followed by the prediction\n",
    "y_pred = clf_best.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# calculate accuracy, Note: y_test is the ground truth for the tesing set\n",
    "# we have similiar score for the testing set as the cross validation score - good\n",
    "\n",
    "print(f'Accuracy Score : {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the Model\n",
    "The following code shows how to save the trained model as a pickle file, which can be loaded in to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf-best.pickle']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle file\n",
    "import joblib\n",
    "joblib.dump(clf_best, \"clf-best.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num_pipeline',\n",
       "                                                  Pipeline(steps=[('num_imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Age', 'SibSp', 'Fare']),\n",
       "                                                 ('cat_pipeline',\n",
       "                                                  Pipeline(steps=[('cat_imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['Sex', 'Pclass'])])),\n",
       "                ('clf_dt',\n",
       "                 DecisionTreeClassifier(criterion='entropy', max_depth=3))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from a pickle file\n",
    "saved_tree_clf = joblib.load(\"clf-best.pickle\")\n",
    "saved_tree_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Sex  Age  SibSp  Fare\n",
       "0       3  male   23      0   5.5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger1 = pd.DataFrame(\n",
    "    {\n",
    "        'Pclass': [3],\n",
    "        'Sex': ['male'], \n",
    "        'Age': [23],\n",
    "        'SibSp': [0],\n",
    "        'Fare': [5.5],\n",
    "    }\n",
    ")\n",
    "passenger1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex  Age  SibSp  Fare\n",
       "0       1  female   21      0    80"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger2 = pd.DataFrame(\n",
    "    {\n",
    "        'Pclass': [1],\n",
    "        'Sex': ['female'], \n",
    "        'Age': [21],\n",
    "        'SibSp': [0],\n",
    "        'Fare': [80],\n",
    "    }\n",
    ")\n",
    "passenger2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# died\n",
    "pred1 = saved_tree_clf.predict(passenger1)\n",
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# survived\n",
    "pred2 = saved_tree_clf.predict(passenger2)\n",
    "pred2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60983cc5b4e6e68ad045e44d0cff1b681bb235747c6ffab0a64a1207595c05a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
